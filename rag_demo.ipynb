{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e415b75c-caf8-425f-92e3-c59b926ef5d6",
   "metadata": {},
   "source": [
    "#  Amazon Bedrock RAG Template Demo \n",
    "\n",
    "This Jupyter notebooks gives a short demonstration of the Bedrock RAG use cas template where Amazon Bedrock invocations augmented with embeddings retrieved from Aurora vector data base. \n",
    "\n",
    "## Agenda:\n",
    "\n",
    "- Installing requirements\n",
    "- Embedding definition\n",
    "- Database connection \n",
    "- Data ingestion\n",
    "- Retrieval augmented text generation\n",
    "- Relevant document queries\n",
    "\n",
    "\n",
    "## Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b45d74-2bf3-43bc-94fc-4f0d6236fa0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.2.1 \n",
    "!pip install langchain-community==0.2.1\n",
    "!pip install pgvector==0.2.5 \n",
    "!pip install psycopg2-binary==2.9.9 \n",
    "!pip install pydantic-settings==2.1.0 \n",
    "!pip install instructor==0.3.5 \n",
    "!pip install tiktoken==0.7.0\n",
    "!pip install boto3==1.34.101 \n",
    "!pip install langchain_aws==0.1.6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f083da-a34f-42d4-afd1-5c04604a8c88",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "### Imports and the creation of the boto3 session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b386f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "from boto3 import Session\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import psycopg2\n",
    "from langchain_community.vectorstores.pgvector import DistanceStrategy, PGVector\n",
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# Configure the logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Use the session to create a client\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c5e4d",
   "metadata": {},
   "source": [
    "### Retrieving environment variables from the SSM parameter store   \n",
    "\n",
    "The Terraform deployment saves all essential environment variables to the AWS SSM parameter store. To retrieve those, we use the following helper function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb916f48-df78-4d17-9f3c-27c1b7b6185a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ssm_parameter(session: Session, parameter_name: str, prefix:str = '/bedrock-rag-template/'):\n",
    "    \"\"\"Retrieve a parameter's value from AWS SSM Parameter Store.\n",
    "\n",
    "    Args:\n",
    "        session (Session): the boto3 session to use to retrieve the parameters\n",
    "        parameter_name (str): the name of the parameter\n",
    "        prefix (str, optional): Parameter's prefix. Defaults to '/bedrock-rag-template/'.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    ssm = session.client('ssm')\n",
    "    response = ssm.get_parameter(\n",
    "        Name = prefix+parameter_name\n",
    "    )\n",
    "    return response['Parameter']['Value']\n",
    "\n",
    "\n",
    "# Setup env variables\n",
    "VECTOR_DB_INDEX = get_ssm_parameter(session, 'VECTOR_DB_INDEX')\n",
    "PG_VECTOR_DB_NAME = get_ssm_parameter(session, 'PG_VECTOR_DB_NAME')\n",
    "PG_VECTOR_PORT = get_ssm_parameter(session, 'PG_VECTOR_PORT')\n",
    "PG_VECTOR_SECRET_ARN = get_ssm_parameter(session, 'PG_VECTOR_SECRET_ARN')\n",
    "PG_VECTOR_DB_HOST = get_ssm_parameter(session, 'PG_VECTOR_DB_HOST')\n",
    "S3_BUCKET_NAME = get_ssm_parameter(session, 'S3_BUCKET_NAME')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b8377",
   "metadata": {},
   "source": [
    "## Create the Amazon Bedrock Embedding\n",
    "\n",
    "\n",
    "**Prerequisite:** Ensure you have requested the access to the Amazon Bedrock models successfully, for details see [Model access](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html).\n",
    "\n",
    "\n",
    "To create the LangChain vector store, we need to provide a LangChain embedding. The id of the embedding model id must be the same used to create the embeddings in the first place, in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f7adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model_id = \"amazon.titan-embed-text-v2:0\" \n",
    "\n",
    "br = session.client(\"bedrock-runtime\")\n",
    "bedrock_embedding = BedrockEmbeddings(client=br, model_id=embedding_model_id)\n",
    "\n",
    "\n",
    "try:\n",
    "    br.invoke_model(**{\n",
    "         \"modelId\": \"amazon.titan-embed-text-v2:0\",\n",
    "         \"contentType\": \"application/json\",\n",
    "         \"accept\": \"*/*\",\n",
    "         \"body\": \"{\\\"inputText\\\":\\\"this is where you place your input text\\\", \\\"dimensions\\\": 512, \\\"normalize\\\": true}\"\n",
    "        })\n",
    "except Exception as e:\n",
    "    logger.error(f\"Please enable model access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ea47a",
   "metadata": {},
   "source": [
    "## Establish a connection the Amazon Aurora and create the LangChain vector store\n",
    "To get the secret for the data base, we use the following helper function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf90da-26ae-4e61-9193-df521635a3c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_db_secret_value(secret_arn: str) -> str:\n",
    "    \"\"\"Get the secret value from the secret manager\n",
    "\n",
    "    Args:\n",
    "        secret_arn (str): ARN of the secret\n",
    "\n",
    "    Returns:\n",
    "        str: Value of the secret\n",
    "    \"\"\"\n",
    "    client = boto3.client('secretsmanager')\n",
    "    get_secret_value_response = client.get_secret_value(SecretId=secret_arn)\n",
    "    return json.loads(get_secret_value_response['SecretString'])\n",
    "\n",
    "\n",
    "logger.info(f\"Retrieve secret from {PG_VECTOR_SECRET_ARN}\")\n",
    "client = session.client(service_name='secretsmanager')\n",
    "credentials = get_db_secret_value(PG_VECTOR_SECRET_ARN)\n",
    "\n",
    "\n",
    "connection_string = PGVector.connection_string_from_db_params(\n",
    "    driver=\"psycopg2\",\n",
    "    host=PG_VECTOR_DB_HOST,\n",
    "    port=PG_VECTOR_PORT,\n",
    "    database=PG_VECTOR_DB_NAME,\n",
    "    user=credentials['username'],\n",
    "    password=credentials['password']\n",
    ")\n",
    "\n",
    "\n",
    "vector_store = PGVector(\n",
    "    connection_string=connection_string,\n",
    "    collection_name=VECTOR_DB_INDEX,\n",
    "    embedding_function=bedrock_embedding,\n",
    "    distance_strategy=DistanceStrategy.COSINE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7801a-4285-47d5-9b4d-f44011f896da",
   "metadata": {},
   "source": [
    "## Add embeddings to the vector store for RAG \n",
    "\n",
    "To make use of ingestion pipeline triggered by Amazon S3 bucket notifications, we take the following file and put it to the Amazon S3 bucket to trigger the ingestion. To validate the ingestion, we look up the latest invocation of the AWS Lambda function to verify execution. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162e39b-c08d-4acd-80d6-164e7629aa38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_content = \"\"\"\"\n",
    "### Company Overview: TechWorldNova Solutions\n",
    "**TechWorldNova Solutions** is an innovative technology firm specializing in artificial intelligence and cloud computing solutions. \n",
    "Since its founding in 2015, TechNova has been at the forefront of technological advancements, providing cutting-edge products and services to a diverse range of industries.\n",
    "\n",
    "### Growth and Revenue Highlights\n",
    "\n",
    "- **2018:**\n",
    "  - **Revenue:** $15 million\n",
    "  - **Growth:** 25%\n",
    "- **2019:**\n",
    "  - **Revenue:** $20 million\n",
    "  - **Growth:** 33%\n",
    "- **2020:**\n",
    "  - **Revenue:** $30 million\n",
    "  - **Growth:** 50%\n",
    "- **2021:**\n",
    "  - **Revenue:** $45 million\n",
    "  - **Growth:** 50%\n",
    "- **2022:**\n",
    "  - **Revenue:** $60 million\n",
    "  - **Growth:** 33%\n",
    "- **2023:**\n",
    "  - **Revenue:** $80 million\n",
    "  - **Growth:** 33%\n",
    "\n",
    "### Key Milestones\n",
    "- **2017:** Launched first AI-powered analytics platform.\n",
    "- **2019:** Expanded operations to Europe and Asia.\n",
    "- **2021:** Introduced cloud computing solutions, gaining significant market traction.\n",
    "- **2023:** Reached 500+ enterprise clients and crossed $80 million in revenue.\n",
    "### Future Outlook\n",
    "\n",
    "TechNova Solutions aims to continue its upward trajectory by investing in research and development, \n",
    "exploring new markets, and enhancing its product offerings. \n",
    "The company's vision is to be a global leader in AI and cloud computing, driving innovation and delivering exceptional value to its clients.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de69ba4-ca18-4cf8-b9e2-faa964c2be88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = session.client(\"s3\")\n",
    "s3.put_object(\n",
    "    Bucket=S3_BUCKET_NAME,\n",
    "    Key=\"rag-template-file.txt\",\n",
    "    Body=file_content.encode('utf-8')\n",
    ")\n",
    "\n",
    "# Wait until documents are in store\n",
    "i = 0\n",
    "while i < 10:\n",
    "    i += 1\n",
    "    \n",
    "    ingested_docs = vector_store.similarity_search(\"TechWorldNova Solutions\")\n",
    "    if len(ingested_docs) > 0:\n",
    "        print(\"Relevant documents found\")\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7367250-e207-4fd9-8d81-a9be5115e85f",
   "metadata": {},
   "source": [
    "### Verfify that embedding is present in vector store\n",
    "\n",
    "We check whether there is a document similar to the string \"TechWorldNova Solutions\" to verify presence of the embedding in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee8dfe-c65c-44c6-90b1-a001ab08747b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store.similarity_search(\"TechWorldNova Solutions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3833ba52-afa3-4e0e-b7ca-a08e7e0459d6",
   "metadata": {},
   "source": [
    "## Retrieval augmented text generation using Bedrock Claude and the PGVector vector store\n",
    "\n",
    "\n",
    "Subsequently, we generate a system prompt to test the retrieval augmentation by storing information about an fictitious company called `TechWorldNova Solutions`. Thereby. We ensure that the foundation model has not been trained on the answer yet. We test the retrieval augmentation with Anthropic Claude 2 and 3. \n",
    "\n",
    "\n",
    "\n",
    "### Prepare the retriever and the system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe0a26-b141-4c92-9a15-03f0f5d13534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "retriever=vector_store.as_retriever(search_type=\"similarity_score_threshold\",\n",
    "                                    search_kwargs={'score_threshold': 0.8})\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "query = \"What is the mission of TechWorldNova Solutions?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f3323e-b31d-41ac-838f-0e844fe7fd0d",
   "metadata": {},
   "source": [
    "### Claude 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c994b83-e065-4bf6-bded-0fba105e86c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "model_kwargs =  { \n",
    "    \"max_tokens\": 2048,  \n",
    "}\n",
    "\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "response = chain.invoke({\"input\": query})[\"answer\"]\n",
    "print(f\"CHATBOT ANSWER CLAUDE 3: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4a2d6-bf7f-460d-a19b-385c82ea2a57",
   "metadata": {},
   "source": [
    "## Retrieve relevant documents for the query (optional)\n",
    "Run the following cell if you want to get more details about the scores of the selected chunks, relevant for answering the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e065c3c-efe0-406a-81d5-9569b960cdda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_scores = vector_store.similarity_search_with_relevance_scores(query, k=20)\n",
    "\n",
    "docs = []\n",
    "for doc, score in doc_scores:\n",
    "    doc.metadata[\"document_score\"] = score\n",
    "    docs.append(doc)\n",
    "\n",
    "for item in docs:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d409d74d-110e-4045-b767-e2c88f7d48c6",
   "metadata": {},
   "source": [
    "## Retrieve the raw data from the vector store (optional)\n",
    "If you want to have explore the raw vector store, you can use the query below which fetches all records (only applicable if a few documents are present in the data base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ff67b-469c-4738-80fa-03b7bff5ead5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host=PG_VECTOR_DB_HOST,\n",
    "            database=PG_VECTOR_DB_NAME,\n",
    "            user=credentials['username'],\n",
    "            password=credentials['password'])\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT * FROM langchain_pg_embedding\")\n",
    "ids = cur.fetchall()\n",
    "\n",
    "# Print metadata:\n",
    "# i[0] - document IDs\n",
    "# i[1] - embeddings\n",
    "# i[2] - plain text documents\n",
    "# i[3] - document metadata\n",
    "\n",
    "print([i[2] for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055dd3cb-303d-4511-a7b6-297180eba38f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
